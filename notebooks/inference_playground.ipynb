{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Third Time's the Charm? StyleGAN3 Inference Notebook"
      ],
      "metadata": {
        "collapsed": false,
        "id": "JaDAfZJxWwRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Environment and Download Code"
      ],
      "metadata": {
        "collapsed": false,
        "id": "wYC3ohBuWwRv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'stylegan3-editing'...\n",
            "remote: Enumerating objects: 278, done.\u001b[K\n",
            "remote: Counting objects: 100% (48/48), done.\u001b[K\n",
            "remote: Compressing objects: 100% (43/43), done.\u001b[K\n",
            "remote: Total 278 (delta 8), reused 5 (delta 5), pack-reused 230 (from 1)\u001b[K\n",
            "Receiving objects: 100% (278/278), 74.09 MiB | 16.04 MiB/s, done.\n",
            "Resolving deltas: 100% (38/38), done.\n",
            "--2025-02-08 16:18:57--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250208T161857Z&X-Amz-Expires=300&X-Amz-Signature=40915d34e979aae05f1874efb141de5969170b00a9607feb1a37111f1ff8d066&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-02-08 16:18:57--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20250208%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20250208T161857Z&X-Amz-Expires=300&X-Amz-Signature=40915d34e979aae05f1874efb141de5969170b00a9607feb1a37111f1ff8d066&X-Amz-SignedHeaders=host&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip’\n",
            "\n",
            "ninja-linux.zip     100%[===================>]  76.03K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2025-02-08 16:18:58 (24.4 MB/s) - ‘ninja-linux.zip’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "  inflating: /usr/local/bin/ninja    \n",
            "update-alternatives: using /usr/local/bin/ninja to provide /usr/bin/ninja (ninja) in auto mode\n",
            "Collecting pyrallis\n",
            "  Downloading pyrallis-0.3.1-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting typing-inspect (from pyrallis)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from pyrallis) (6.0.2)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyrallis)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from typing-inspect->pyrallis) (4.12.2)\n",
            "Downloading pyrallis-0.3.1-py3-none-any.whl (33 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, typing-inspect, pyrallis\n",
            "Successfully installed mypy-extensions-1.0.0 pyrallis-0.3.1 typing-inspect-0.9.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ye4gbub8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ye4gbub8\n",
            "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.20.1+cu124)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m97.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369489 sha256=885f7aaf4d863f4d85b00f71e236db7c4aefb317503b00ec0d1c5c99b28109dd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-p9p26uk6/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed clip-1.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "#@title Clone Repo and Install Ninja { display-mode: \"form\" }\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "os.chdir('/content')\n",
        "CODE_DIR = 'stylegan3-editing'\n",
        "\n",
        "## clone repo\n",
        "!git clone https://github.com/yuval-alaluf/stylegan3-editing $CODE_DIR\n",
        "\n",
        "## install ninja\n",
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force\n",
        "\n",
        "## install some packages\n",
        "!pip install pyrallis\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "os.chdir(f'./{CODE_DIR}')"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "f6yNEkgqWwRw",
        "outputId": "7ccdde7f-4942-4430-e072-a2ffea1e0866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/cpp_extension.py:1964: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
            "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "#@title Import Packages { display-mode: \"form\" }\n",
        "import time\n",
        "import sys\n",
        "import pprint\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import dataclasses\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "from editing.interfacegan.face_editor import FaceEditor\n",
        "from editing.styleclip_global_directions import edit as styleclip_edit\n",
        "from models.stylegan3.model import GeneratorType\n",
        "from notebooks.notebook_utils import Downloader, ENCODER_PATHS, INTERFACEGAN_PATHS, STYLECLIP_PATHS\n",
        "from notebooks.notebook_utils import run_alignment, crop_image, compute_transforms\n",
        "from utils.common import tensor2im\n",
        "from utils.inference_utils import run_on_batch, load_encoder, get_average_image\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "uHqAqmezWwRw",
        "outputId": "6774ba36-6716-442a-8ef7-a364a034e222",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Download Configuration\n",
        "Select below whether you wish to download all models using `pydrive`. Note that if you do not use `pydrive`, you may encounter a \"quota exceeded\" error from Google Drive."
      ],
      "metadata": {
        "collapsed": false,
        "id": "woAwSOAaWwRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "download_with_pydrive = True #@param {type:\"boolean\"}\n",
        "downloader = Downloader(code_dir=CODE_DIR,\n",
        "                        use_pydrive=download_with_pydrive,\n",
        "                        subdir=\"pretrained_models\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "a6fCWPMmWwRx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select Model for Inference\n",
        "Currently, we have ReStyle-pSp and ReStyle-e4e encoders trained for human faces."
      ],
      "metadata": {
        "collapsed": false,
        "id": "M68NLqfjWwRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Select which model/domain you wish to perform inference on: { display-mode: \"form\" }\n",
        "experiment_type = 'restyle_pSp_ffhq' #@param ['restyle_e4e_ffhq', 'restyle_pSp_ffhq']"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "w4T4ONfZWwRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Inference Parameters\n",
        "\n",
        "Below we have a dictionary defining parameters such as the path to the pretrained model to use and the path to the image to perform inference on. While we provide default values to run this script, feel free to change as needed."
      ],
      "metadata": {
        "collapsed": false,
        "id": "DRjhSecWWwRy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "EXPERIMENT_DATA_ARGS = {\n",
        "    \"restyle_pSp_ffhq\": {\n",
        "        \"model_path\": \"./pretrained_models/restyle_pSp_ffhq.pt\",\n",
        "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    },\n",
        "    \"restyle_e4e_ffhq\": {\n",
        "        \"model_path\": \"./pretrained_models/restyle_e4e_ffhq.pt\",\n",
        "        \"image_path\": \"./notebooks/images/face_image.jpg\",\n",
        "        \"transform\": transforms.Compose([\n",
        "            transforms.Resize((256, 256)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])])\n",
        "    }\n",
        "}\n",
        "\n",
        "EXPERIMENT_ARGS = EXPERIMENT_DATA_ARGS[experiment_type]"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "9gP3JjSzWwRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Models\n",
        "To reduce the number of requests to fetch the model, we'll check if the model was previously downloaded and saved before downloading the model.\n",
        "We'll download the model for the selected experiment and save it to the folder `stylegan3-editing/pretrained_models`.\n",
        "\n",
        "We also need to verify that the model was downloaded correctly.\n",
        "Note that if the file weighs several KBs, you most likely encounter a \"quota exceeded\" error from Google Drive."
      ],
      "metadata": {
        "collapsed": false,
        "id": "q3VaM28sWwRy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Download ReStyle SG3 Encoder { display-mode: \"form\" }\n",
        "if not os.path.exists(EXPERIMENT_ARGS['model_path']) or os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
        "    print(f'Downloading ReStyle encoder model: {experiment_type}...')\n",
        "    try:\n",
        "      downloader.download_file(file_id=ENCODER_PATHS[experiment_type]['id'],\n",
        "                              file_name=ENCODER_PATHS[experiment_type]['name'])\n",
        "    except Exception as e:\n",
        "      raise ValueError(f\"Unable to download model correctly! {e}\")\n",
        "    # if google drive receives too many requests, we'll reach the quota limit and be unable to download the model\n",
        "    if os.path.getsize(EXPERIMENT_ARGS['model_path']) < 1000000:\n",
        "        raise ValueError(\"Pretrained model was unable to be downloaded correctly!\")\n",
        "    else:\n",
        "        print('Done.')\n",
        "else:\n",
        "    print(f'Model for {experiment_type} already exists!')\n"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "OHSJHwaNWwRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pretrained Model\n",
        "We assume that you have downloaded all relevant models and placed them in the directory defined by the\n",
        "`EXPERIMENT_DATA_ARGS` dictionary."
      ],
      "metadata": {
        "collapsed": false,
        "id": "Ssd_B5gDWwRz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Load ReStyle SG3 Encoder { display-mode: \"form\" }\n",
        "model_path = EXPERIMENT_ARGS['model_path']\n",
        "net, opts = load_encoder(checkpoint_path=model_path)\n",
        "pprint.pprint(dataclasses.asdict(opts))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "L7Q1YP-tWwRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Inputs\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "YZna_HFwWwRz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Define and Visualize Input { display-mode: \"form\" }\n",
        "\n",
        "image_path = Path(EXPERIMENT_DATA_ARGS[experiment_type][\"image_path\"])\n",
        "original_image = Image.open(image_path).convert(\"RGB\")\n",
        "original_image = original_image.resize((256, 256))\n",
        "original_image"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qzoN1ereWwRz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Get Aligned and Cropped Input Images\n",
        "input_image = run_alignment(image_path)\n",
        "cropped_image = crop_image(image_path)\n",
        "joined = np.concatenate([input_image.resize((256, 256)), cropped_image.resize((256, 256))], axis=1)\n",
        "Image.fromarray(joined)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4ETjGDp3WwRz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Compute Landmarks-Based Transforms { display-mode: \"form\" }\n",
        "images_dir = Path(\"./images\")\n",
        "images_dir.mkdir(exist_ok=True, parents=True)\n",
        "cropped_path = images_dir / f\"cropped_{image_path.name}\"\n",
        "aligned_path = images_dir / f\"aligned_{image_path.name}\"\n",
        "cropped_image.save(cropped_path)\n",
        "input_image.save(aligned_path)\n",
        "landmarks_transform = compute_transforms(aligned_path=aligned_path, cropped_path=cropped_path)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QtMh5Ba8WwRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform Inversion\n",
        "Now we'll run inference. By default, we'll run using 3 inference steps. You can change the parameter in the cell below."
      ],
      "metadata": {
        "collapsed": false,
        "id": "WrsvPRRuWwR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title { display-mode: \"form\" }\n",
        "n_iters_per_batch = 3 #@param {type:\"integer\"}\n",
        "opts.n_iters_per_batch = n_iters_per_batch\n",
        "opts.resize_outputs = False  # generate outputs at full resolution"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "BqCIKpLAWwR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Run Inference { display-mode: \"form\" }\n",
        "img_transforms = EXPERIMENT_ARGS['transform']\n",
        "transformed_image = img_transforms(input_image)\n",
        "\n",
        "avg_image = get_average_image(net)\n",
        "\n",
        "with torch.no_grad():\n",
        "    tic = time.time()\n",
        "    result_batch, result_latents = run_on_batch(inputs=transformed_image.unsqueeze(0).cuda().float(),\n",
        "                                                net=net,\n",
        "                                                opts=opts,\n",
        "                                                avg_image=avg_image,\n",
        "                                                landmarks_transform=torch.from_numpy(landmarks_transform).cuda().float())\n",
        "    toc = time.time()\n",
        "    print('Inference took {:.4f} seconds.'.format(toc - tic))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "Wp1bXZxaWwR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Visualize Result { display-mode: \"form\" }\n",
        "\n",
        "def get_coupled_results(result_batch, cropped_image):\n",
        "    result_tensors = result_batch[0]  # there's one image in our batch\n",
        "    resize_amount = (256, 256) if opts.resize_outputs else (opts.output_size, opts.output_size)\n",
        "    final_rec = tensor2im(result_tensors[-1]).resize(resize_amount)\n",
        "    input_im = cropped_image.resize(resize_amount)\n",
        "    res = np.concatenate([np.array(input_im), np.array(final_rec)], axis=1)\n",
        "    res = Image.fromarray(res)\n",
        "    return res\n",
        "\n",
        "res = get_coupled_results(result_batch, cropped_image)\n",
        "res.resize((1024, 512))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "GQjaRZakWwR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Save Result { display-mode: \"form\" }\n",
        "\n",
        "# save image\n",
        "outputs_path = \"./outputs\"\n",
        "os.makedirs(outputs_path, exist_ok=True)\n",
        "res.save(os.path.join(outputs_path, os.path.basename(image_path)))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "O7GfuV-zWwR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Editing\n",
        "Given the resulting latent code obtained above, we can perform various edits, which we demonstrate below."
      ],
      "metadata": {
        "collapsed": false,
        "id": "B23hu6m1WwR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Download pretrained boundaries and editing files: { display-mode: \"form\" }\n",
        "download_with_pydrive = True #@param {type:\"boolean\"}\n",
        "\n",
        "# download files for interfacegan\n",
        "downloader = Downloader(code_dir=CODE_DIR,\n",
        "                        use_pydrive=download_with_pydrive,\n",
        "                        subdir=\"editing/interfacegan/boundaries/ffhq\")\n",
        "print(\"Downloading InterFaceGAN boundaries...\")\n",
        "for editing_file, params in INTERFACEGAN_PATHS.items():\n",
        "    print(f\"Downloading {editing_file} boundary...\")\n",
        "    downloader.download_file(file_id=params['id'],\n",
        "                             file_name=params['name'])\n",
        "\n",
        "# download files for styleclip\n",
        "downloader = Downloader(code_dir=CODE_DIR,\n",
        "                        use_pydrive=download_with_pydrive,\n",
        "                        subdir=\"editing/styleclip_global_directions/sg3-r-ffhq-1024\")\n",
        "print(\"Downloading StyleCLIP auxiliary files...\")\n",
        "for editing_file, params in STYLECLIP_PATHS.items():\n",
        "    print(f\"Downloading {editing_file}...\")\n",
        "    downloader.download_file(file_id=params['id'],\n",
        "                             file_name=params['name'])"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "CQkEWFwLWwR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### InterFaceGAN"
      ],
      "metadata": {
        "collapsed": false,
        "id": "iGeHRfH3WwR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "editor = FaceEditor(stylegan_generator=net.decoder, generator_type=GeneratorType.ALIGNED)\n",
        "\n",
        "#@title Select which edit you wish to perform: { display-mode: \"form\" }\n",
        "edit_direction = 'age' #@param ['age', 'smile', 'pose', 'Male']\n",
        "min_value = -5 #@param {type:\"slider\", min:-10, max:10, step:1}\n",
        "max_value = 5 #@param {type:\"slider\", min:-10, max:10, step:1}"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gluCrrahWwR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Perform Edit! { display-mode: \"form\" }\n",
        "print(f\"Performing edit for {edit_direction}...\")\n",
        "input_latent = torch.from_numpy(result_latents[0][-1]).unsqueeze(0).cuda()\n",
        "edit_images, edit_latents = editor.edit(latents=input_latent,\n",
        "                                        direction=edit_direction,\n",
        "                                        factor_range=(min_value, max_value),\n",
        "                                        user_transforms=landmarks_transform,\n",
        "                                        apply_user_transformations=True)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "gM-Ks4myWwR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Show Result { display-mode: \"form\" }\n",
        "def prepare_edited_result(edit_images):\n",
        "  if type(edit_images[0]) == list:\n",
        "      edit_images = [image[0] for image in edit_images]\n",
        "  res = np.array(edit_images[0].resize((512, 512)))\n",
        "  for image in edit_images[1:]:\n",
        "      res = np.concatenate([res, image.resize((512, 512))], axis=1)\n",
        "  res = Image.fromarray(res).convert(\"RGB\")\n",
        "  return res\n",
        "\n",
        "res = prepare_edited_result(edit_images)\n",
        "res"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "QQOj1m2HWwR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### StyleCLIP (Global Directions)"
      ],
      "metadata": {
        "collapsed": false,
        "id": "fONDm3XpWwR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Prepare StyleCLIP Editor { display-mode: \"form\" }\n",
        "styleclip_args = styleclip_edit.EditConfig()\n",
        "global_direction_calculator = styleclip_edit.load_direction_calculator(stylegan_model=net.decoder, opts=styleclip_args)"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "HBx_zcwMWwR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Select which edit you wish to perform. { display-mode: \"form\" }\n",
        "#@markdown We recommend keeping the neutral_text as is by default.\n",
        "neutral_text = \"a face\" #@param {type:\"raw\"}\n",
        "target_text = \"a smiling face\" #@param {type:\"raw\"}\n",
        "alpha = 4 #@param {type:\"slider\", min:-5, max:5, step:0.5}\n",
        "beta = 0.13 #@param {type:\"slider\", min:-1, max:1, step:0.1}"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "ho49_p-zWwR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Perform Edit! { display-mode: \"form\" }\n",
        "opts = styleclip_edit.EditConfig()\n",
        "opts.alpha_min = alpha\n",
        "opts.alpha_max = alpha\n",
        "opts.num_alphas = 1\n",
        "opts.beta_min = beta\n",
        "opts.beta_max = beta\n",
        "opts.num_betas = 1\n",
        "opts.neutral_text = neutral_text\n",
        "opts.target_text = target_text\n",
        "\n",
        "input_latent = result_latents[0][-1]\n",
        "input_transforms = torch.from_numpy(landmarks_transform).cpu().numpy()\n",
        "print(f'Performing edit for: \"{opts.target_text}\"...')\n",
        "edit_res, edit_latent = styleclip_edit.edit_image(latent=input_latent,\n",
        "                                                  landmarks_transform=input_transforms,\n",
        "                                                  stylegan_model=net.decoder,\n",
        "                                                  global_direction_calculator=global_direction_calculator,\n",
        "                                                  opts=opts,\n",
        "                                                  image_name=None,\n",
        "                                                  save=False)\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UH9TmXsBWwR2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Show Result { display-mode: \"form\" }\n",
        "input_im = tensor2im(transformed_image).resize((512, 512))\n",
        "edited_im = tensor2im(edit_res[0]).resize((512, 512))\n",
        "edit_coupled = np.concatenate([np.array(input_im), np.array(edited_im)], axis=1)\n",
        "edit_coupled = Image.fromarray(edit_coupled)\n",
        "edit_coupled.resize((1024, 512))"
      ],
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "b-M4GF2IWwR2"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "inference_playground (5).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}